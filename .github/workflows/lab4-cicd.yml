name: Lab4 Automated Model Deployment (CI/CD)

on:
  push:
    branches: ["main"]

jobs:
  train:
    name: Train Model (CI)
    runs-on: ubuntu-latest

    outputs:
      mse: ${{ steps.read_metrics.outputs.mse }}
      r2: ${{ steps.read_metrics.outputs.r2 }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      - name: Run training
        run: |
          python train.py

      - name: Read metrics
        id: read_metrics
        run: |
          echo "mse=$(python -c "import json; print(json.load(open('artifacts/metrics.json'))['mse'])")" >> $GITHUB_OUTPUT
          echo "r2=$(python -c "import json; print(json.load(open('artifacts/metrics.json'))['r2'])")" >> $GITHUB_OUTPUT

      - name: Upload Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: model_artifacts
          path: |
            artifacts/model.pkl
            artifacts/metrics.json

  deploy:
    name: Deploy Model (CD)
    needs: train
    runs-on: ubuntu-latest

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Download Artifacts
        uses: actions/download-artifact@v4
        with:
          name: model_artifacts
          path: artifacts

      - name: Verify downloaded files
        run: |
          echo "Listing artifacts folder:"
          ls -R artifacts
          test -f artifacts/model.pkl
          test -f artifacts/metrics.json

      - name: Print metrics
        run: |
          echo "Current metrics:"
          cat artifacts/metrics.json
          echo "BEST_MSE=${{ vars.BEST_MSE }}"
          echo "BEST_R2=${{ vars.BEST_R2 }}"

      - name: Compare metrics (stop if no improvement)
        id: compare
        run: |
          python - <<'PY'
          import json, os

          cur = json.load(open("artifacts/metrics.json"))
          cur_mse = float(cur["mse"])
          cur_r2 = float(cur["r2"])

          best_mse = float(os.environ["BEST_MSE"])
          best_r2  = float(os.environ["BEST_R2"])

          improved = (cur_r2 > best_r2) and (cur_mse < best_mse)

          print("Current:", cur_mse, cur_r2)
          print("Best:", best_mse, best_r2)

          if improved:
              print("Model improved ")
              print("improved=true")
          else:
              print("2022BCS0185 ---- Metric did not improve")
              print("improved=false")
          PY
        env:
          BEST_MSE: ${{ vars.BEST_MSE }}
          BEST_R2: ${{ vars.BEST_R2 }}

      - name: Stop deploy if not improved
        run: |
          echo "Skipping docker build/push because metric did not improve."
          exit 0
        if: ${{ !contains(steps.compare.outputs.improved, 'true') }}

      - name: Login to Docker Hub
        if: ${{ contains(steps.compare.outputs.improved, 'true') }}
        run: echo "${{ secrets.DOCKER_TOKEN }}" | docker login -u "${{ secrets.DOCKER_USERNAME }}" --password-stdin

      - name: Build Image
        if: ${{ contains(steps.compare.outputs.improved, 'true') }}
        run: |
          docker build -t ${{ secrets.DOCKER_USERNAME }}/wine_predict_2022bcs0185:latest .

      - name: Push Image
        if: ${{ contains(steps.compare.outputs.improved, 'true') }}
        run: |
          docker push ${{ secrets.DOCKER_USERNAME }}/wine_predict_2022bcs0185:latest
